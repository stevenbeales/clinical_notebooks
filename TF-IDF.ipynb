{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF-IDF to match inclusion criteria with specific patient clinical criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanzir Hasan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From clinicaltrails.gov all the clinical trails data for ovarian tumor were downloaded in xml format. The inclusion and exclusion criteria of all the trails were carefully examined. There were some trail data that had seperate inclsion  and exclusion criteria in sperate heading. But there were some trail data that had no sperate heading. For purpose of analysis the file that had seperate inclusion and exclusion criteria were coppied to a seperate folder and trials with gerertal elegibility criteria were coppied to a sperate folder. For this purpose the following python code was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "filenames = os.listdir('/Users/tanzir6/Documents/NLP_Project/search_result')\n",
    "urls =[]\n",
    "\n",
    "dts1 = '/Users/tanzir6/Documents/NLP_Project/Inclusion'\n",
    "dts2 = '/Users/tanzir6/Documents/NLP_Project/General'\n",
    "for name in filenames:\n",
    "    if name.endswith('xml'):\n",
    "        urls.append(name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pat= re.compile(r'\\s+')\n",
    "\n",
    "for url in urls:\n",
    "    txt = pat.sub(' ',(BeautifulSoup(open(url).read(),\"html5lib\")).find('criteria').text.lower())\n",
    "    txt = txt.encode('utf-8')\n",
    "\n",
    "    if 'exclusion criteria' in txt:\n",
    "        copyfile(url,  os.path.join(dts1,url))\n",
    "    else:\n",
    "        copyfile(url, os.path.join(dts2,url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the  files that had seperate inclusion and exclusion criteria, inclusion criteria and exclusion are separated as two text string. This  text string of inclusion criteria is then compared with a text string containig patient's clinical criteria. For comprison both the  documents are transformed into tf-idf vector, then compute the cosine similarity between them. Inclusion criteria from all the file is thus compared with the test clinical criteria. And top ten match of the clinical trail are returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import nltk.corpus\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "vectorizer = TfidfVectorizer(tokenizer= normalize, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def normalize(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(text1, text2):\n",
    "    tfidf= vectorizer.fit_transform([text1, text2])\n",
    "    return ((tfidf * tfidf.T).A)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test clinical criteria is taken as text_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_test='Ovarian cancer confirmed by biopsy. Figo stage iv.Indwelling venous acess\\\n",
    "present.patients is able to give written informed consent.Newly diagnosed epithelial ovarian cancer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the file name from the folder containing all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allfiles = os.listdir('/Users/tanzir6/Documents/NLP_Project/Inclusion')\n",
    "filenames =[]\n",
    "for name in allfiles:\n",
    "    if name.endswith('xml'):\n",
    "        filenames.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compling pattern for using regular expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pat = re.compile(r'\\s+')\n",
    "pat2 = re.compile(r'- inclusion criteria:|inclusion criteria:|inclusion criteria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "match = {}\n",
    "for file in filenames:\n",
    "    txt = BeautifulSoup(open(os.path.join(\"/Users/tanzir6/Documents/NLP_Project/Inclusion\",file)).read(),\"html5lib\").find('criteria').text\n",
    "    in_cr = pat.sub(' ',txt.lower().split('exclusion criteria:',1)[0])\n",
    "    txt_string = pat2.sub(' ',in_cr)\n",
    "    match[str(file).strip('.xml')] = cosine_sim(txt_string,text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top ten match of the clinical trails with the given clinical criteria "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCT00155896 0.487375111852\n",
      "NCT02062697 0.398760670081\n",
      "NCT02518256 0.398760670081\n",
      "NCT01470235 0.391975226235\n",
      "NCT02970786 0.379562815505\n",
      "NCT02524808 0.35142674059\n",
      "NCT02711865 0.345693834649\n",
      "NCT00463996 0.345693834649\n",
      "NCT02222883 0.33345467411\n",
      "NCT02122588 0.320731788784\n"
     ]
    }
   ],
   "source": [
    "for w in sorted (match, key= match.get, reverse= True)[:10]:\n",
    "    print w,match[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
